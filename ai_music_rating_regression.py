# -*- coding: utf-8 -*-
"""Clean_AI_Music_Rating.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18gG2i4xecMm4MAgBmyucPaJUpQe0RBBV
"""

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
from tqdm import tqdm
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

# Make sure the file is uploaded and available here
os.listdir("./")

# Load the dataset
df = pd.read_csv("review_dataset.csv")

# View the dataset
df.head(10)

"""## Processing the dataset

### 1. Get an overview of the dataset
"""

df.info()

# Print names of columns which will make our features and labels
df.columns

"""### 2. Identify the numerical and categorical"""

# Identify the numerical and categorical features along with the target feature
numerical_features = ["time_signature", "speechiness", "danceability", "duration_ms", "energy", "mode", "instrumentalness", "valence", "key", "tempo", "loudness", "acousticness", "liveness"]

categorical_features = [
    "playlist_subgenre",
    "playlist_genre",
    "type",
]

model_features = numerical_features + categorical_features
model_target = "track_popularity"

"""## 3.Processing numerical features

We will process numerical features in two ways:
1. Handling missing values
2. Feature scaling

Features being on different scales can affect ML algorithms. Feature scaling involves converting all numerical features to the same scale.

Particularly, scaling helps the most when multiple features have a range that are much different and don't overlap.

sklearn's MinMaxScaler class, which transforms all the numerical features to values within the range of 0 and 1.
"""

# Check missing value counts
print(f"Shape of the dataset is: {df.shape}")
df.isna().sum()

# df_missing_dropped = df.drop(columns=['Name']).dropna()
# print(f"Shape of the dataset is: {df_missing_dropped.shape}")
# df_missing_dropped.isna().sum()

# Let's drop the missing values and observe the same again
df_missing_dropped = df.dropna()
print(f"Shape of the dataset is: {df_missing_dropped.shape}")
df_missing_dropped.isna().sum()
# df_missing_dropped = df

# Feature scaling
from sklearn.preprocessing import MinMaxScaler

# Define the scaler
feature_scaler = MinMaxScaler()
scaled_features = feature_scaler.fit_transform(df_missing_dropped[numerical_features])
# Scale the features
df_scaled_numerical_features = pd.DataFrame(scaled_features, columns=numerical_features)
df_scaled_numerical_features.head()

"""## 3.Processing categorical features

"""

# Import the OneHotEncoder class
from sklearn.preprocessing import OneHotEncoder

# Define the encoder
categorical_encoder = OneHotEncoder(
    handle_unknown="ignore"
)  # handle_unknown tells the function to ignore any value that was not present in the initial training set
encoded_categoricals = categorical_encoder.fit_transform(
    df_missing_dropped[categorical_features]
)

encoded_categoricals.shape

"""## Bringing it all together


* __Numerical features pipeline:__ Impute missing values with the mean by using sklearn's SimpleImputer, followed by a MinMaxScaler. If different processing is desired for different numerical features, different pipelines should be built as described for the text features pipeline. See the `numerical_processor` in the following code cell.

* __Categoricals pipeline:__ Impute with a placeholder value (this won't have an effect because you already encoded the 'nan' values), and encode with sklearn's OneHotEncoder. If computing memory is an issue, it is a good idea to check the number of unique values for the categoricals to get an estimate of how many dummy features one-hot encoding will create. Note the `handle_unknown` parameter, which tells the encoder to ignore (rather than throw an error for) any unique value that might show in the validation or test set that was not present in the initial training set. See the `categorical_processor` in the following code cell.

Finally, the selective preparations of the dataset features are then put together into a collective ColumnTransformer, which is used in a pipeline along with an estimator. This ensures that the transforms are performed automatically in all situations. This includes on the raw data when fitting the model, when making predictions, when evaluating the model on a validation dataset through cross-validation, or when making predictions on a test dataset in the future.
"""

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

### COLUMN_TRANSFORMER ###
##########################

# Preprocess the numerical features
numerical_processor = Pipeline(
    [
        ("num_imputer", SimpleImputer(strategy="mean")),
        (
            "num_scaler",
            MinMaxScaler(),
        ),  # Shown in case it is needed. Not a must with decision trees.
    ]
)

# Preprocess the categorical features
categorical_processor = Pipeline(
    [
        (
            "cat_imputer",
            SimpleImputer(strategy="constant", fill_value="missing"),
        ),  # Shown in case it is needed. No effect because you already imputed with 'nan' strings.
        (
            "cat_encoder",
            OneHotEncoder(handle_unknown="ignore"),
        ),  # handle_unknown tells it to ignore (rather than throw an error for) any value that was not present in the initial training set.
    ]
)

# Combine all data preprocessors (add more if you choose to define more)
# For each processor/step, specify: a name, the actual process, and the features to be processed.
data_processor = ColumnTransformer(
    [
        ("numerical_processing", numerical_processor, numerical_features),
        ("categorical_processing", categorical_processor, categorical_features),
    ]
)

# Visualize the data processing pipeline
from sklearn import set_config

set_config(display="diagram")
data_processor

"""## Training a classifier

"""

from sklearn.linear_model import LinearRegression # Import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

# Assuming 'track_popularity' ranges from 0 to 100
# Create a dictionary with weights for each class
class_weights = {i: 1 for i in range(101)}  # Initialize with weight 1 for all classes

# Increase weight for higher popularity (e.g., above 80)
for i in range(80, 101):
    class_weights[i] = 20  # Assign higher weight to popular tracks

### PIPELINE ###
################

# Pipeline with all desired data transformers, along with an estimator
# Later, you can set/reach the parameters by using the names issued - for hyperparameter tuning, for example
pipeline = Pipeline(
    [
        ("data_processing", data_processor),
        (
            #"lg",
            #LogisticRegression(
            #    solver="liblinear", penalty="l1", C=0.001, class_weight=class_weights
            #),

            "rf", RandomForestRegressor(n_estimators=100, random_state=42)

            #"gbm", GradientBoostingRegressor(n_estimators=100, random_state=42)
        ),
    ]
)

# Visualize the pipeline
# This will be helpful especially when building more complex pipelines, stringing together multiple preprocessing steps
from sklearn import set_config

set_config(display="diagram")
pipeline

"""### Model training

Before training, we first need to ensure we have the training and test split.
"""

import pandas as pd

def remove_outliers_iqr(df, column):
  """Removes outliers from a dataframe using the IQR method.

  Args:
    df: The dataframe to remove outliers from.
    column: The column to remove outliers from.

  Returns:
    The dataframe with outliers removed.
  """
  Q1 = df[column].quantile(0.25)
  Q3 = df[column].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Remove outliers from numerical features using IQR
for c in numerical_features:
    df = remove_outliers_iqr(df, c)

from sklearn.model_selection import train_test_split

train_data, test_data = train_test_split(df, test_size=0.1, shuffle=True, random_state=42)  # Reduced test_size to 0.001

# Print the shapes of the training, validation, and test datasets
print(
    "Train - Test dataset shapes: ",
    train_data.shape,
    test_data.shape,
)

# Get training data to train the classifier
X_train = train_data[model_features]
y_train = train_data[model_target]

# Fit the classifier to the training data
# Training data going through the pipeline is imputed (with means from the training data),
#   scaled (with the min/max from the training data),
#   and finally used to fit the model
pipeline.fit(X_train, y_train)

"""### Model Evaluation"""

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Get validation data to validate the classifier
X_test = test_data[model_features]
y_test = test_data[model_target]

# Use the fitted model to make predictions on the test dataset
test_predictions = pipeline.predict(X_test)

print("Model performance on the test set:")
print("Mean Squared Error (MSE):", mean_squared_error(y_test, test_predictions))
print("R-squared:", r2_score(y_test, test_predictions))

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, test_predictions)

# Calculate Percentage Error
percentage_error = (mae / y_test.mean()) * 100  # Calculate percentage error

print("Mean Absolute Error (MAE):", mae)  # Print MAE
print("Percentage Error:", percentage_error)  # Print percentage error


# If you want to evaluate classification metrics, you need to convert the continuous predictions to discrete class labels:
# For example, if 'track_popularity' is a continuous variable ranging from 0 to 100,
# you could consider a popularity above 70 as class 1 (popular) and below 70 as class 0 (not popular):
# test_predictions_class = [1 if pred > 70 else 0 for pred in test_predictions]
# Then you can use the classification metrics:
# print(confusion_matrix(y_test, test_predictions_class))
# print(classification_report(y_test, test_predictions_class))
# print("Test accuracy:", accuracy_score(y_test, test_predictions_class))



import matplotlib.pyplot as plt
import numpy as np

# ... (Your existing code for data processing, model training, and evaluation) ...

# Get feature importances
importances = pipeline.named_steps["rf"].feature_importances_

# Get feature names from the data processor
feature_names = (
    pipeline.named_steps["data_processing"]
    .named_transformers_["categorical_processing"]
    .named_steps["cat_encoder"]
    .get_feature_names_out(categorical_features)
    .tolist()
    + numerical_features
)

# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]

# Create plot
plt.figure(figsize=(12, 6))
plt.title("Feature Importance")
# Fix: Use len(importances) or len(feature_names) instead of X_train.shape[1]
plt.bar(range(len(importances)), importances[indices], align="center")
plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)
plt.xlim([-1, len(importances)])
plt.tight_layout()
plt.show()